---
phase: 03-history-analytics
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - dbt/models/staging/events/stg_events__set_logged.sql
  - dbt/models/staging/events/stg_events__workout_started.sql
  - dbt/models/staging/events/stg_events__workout_completed.sql
  - dbt/macros/calculate_1rm.sql
  - dbt/macros/detect_anomaly.sql
  - dbt/macros/filter_exercise_by_gym.sql
autonomous: true

must_haves:
  truths:
    - "Staging models extract set data from raw events"
    - "1RM calculation macro uses Epley formula"
    - "Anomaly detection macro calculates percent change"
    - "Gym filtering macro centralizes global vs gym-specific logic"
  artifacts:
    - path: "dbt/models/staging/events/stg_events__set_logged.sql"
      provides: "Parsed set_logged events with weight, reps, exercise_id"
      contains: "payload"
    - path: "dbt/macros/calculate_1rm.sql"
      provides: "Epley formula macro"
      contains: "30.0"
    - path: "dbt/macros/detect_anomaly.sql"
      provides: "Percent change anomaly detection"
      contains: "0.50"
  key_links:
    - from: "dbt/models/staging/events/stg_events__set_logged.sql"
      to: "events table"
      via: "JSON payload extraction"
      pattern: "payload"
---

<objective>
Create dbt staging models for workout events and reusable macros for analytics calculations.

Purpose: Establishes the foundation for Phase 3 analytics - staging models parse raw events, macros provide single source of truth for 1RM, anomaly detection, and gym-specific filtering (DATA-10).
Output: 3 staging models + 3 dbt macros ready for intermediate model consumption.
</objective>

<execution_context>
@/home/dev/.claude/get-shit-done/workflows/execute-plan.md
@/home/dev/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-history-analytics/03-RESEARCH.md

# Existing dbt structure
@dbt/models/staging/events/_events__sources.yml
@dbt/models/staging/events/stg_events__exercise_created.sql
@dbt/models/staging/events/base/base_events__all.sql

# Event types
@src/types/events.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create staging models for workout events</name>
  <files>
    dbt/models/staging/events/stg_events__set_logged.sql
    dbt/models/staging/events/stg_events__workout_started.sql
    dbt/models/staging/events/stg_events__workout_completed.sql
  </files>
  <action>
Create three staging models following existing patterns (see stg_events__exercise_created.sql):

1. **stg_events__set_logged.sql** - Extract from set_logged events:
   - set_id (payload->>'set_id')
   - workout_id (payload->>'workout_id')
   - exercise_id (payload->>'exercise_id')
   - original_exercise_id (payload->>'original_exercise_id')
   - weight_kg (CAST payload->>'weight_kg' AS DECIMAL)
   - reps (CAST payload->>'reps' AS INTEGER)
   - rir (NULLIF, CAST payload->>'rir' AS INTEGER)
   - logged_at (payload->>'logged_at' or use _created_at)
   - _event_id, _created_at from base

2. **stg_events__workout_started.sql** - Extract:
   - workout_id
   - template_id
   - gym_id
   - started_at
   - _event_id, _created_at

3. **stg_events__workout_completed.sql** - Extract:
   - workout_id
   - completed_at
   - _event_id, _created_at

Use source('raw', 'events') and filter by event_type.
Follow DuckDB JSON extraction: payload->>'field' syntax.
  </action>
  <verify>Run `cd dbt && dbt compile --select stg_events__set_logged stg_events__workout_started stg_events__workout_completed` - should compile without errors</verify>
  <done>Three staging models exist and compile successfully</done>
</task>

<task type="auto">
  <name>Task 2: Create dbt macros for analytics calculations</name>
  <files>
    dbt/macros/calculate_1rm.sql
    dbt/macros/detect_anomaly.sql
    dbt/macros/filter_exercise_by_gym.sql
  </files>
  <action>
Create three macros following dbt conventions:

1. **calculate_1rm.sql** - Epley formula for estimated 1RM:
```sql
{% macro calculate_1rm(weight_column, reps_column) %}
    CASE
        WHEN {{ reps_column }} BETWEEN 1 AND 15
        THEN {{ weight_column }} * (1 + ({{ reps_column }} / 30.0))
        ELSE NULL  -- Formula not accurate outside 1-15 rep range
    END
{% endmacro %}
```

2. **detect_anomaly.sql** - Percent change calculation for anomaly detection:
```sql
{% macro detect_anomaly(current_weight, previous_weight, threshold=0.50) %}
    CASE
        WHEN {{ previous_weight }} IS NULL THEN false
        WHEN {{ previous_weight }} = 0 THEN false
        WHEN ABS(({{ current_weight }} - {{ previous_weight }}) / {{ previous_weight }}) > {{ threshold }}
        THEN true
        ELSE false
    END
{% endmacro %}
```

3. **filter_exercise_by_gym.sql** - Centralized gym-specific filtering logic (HIST-02):
```sql
{% macro filter_exercise_by_gym(exercise_is_global, exercise_gym_id, workout_gym_id) %}
    CASE
        WHEN {{ exercise_is_global }} = true THEN true
        WHEN {{ exercise_is_global }} = false
            AND {{ exercise_gym_id }} = {{ workout_gym_id }}
            THEN true
        ELSE false
    END
{% endmacro %}
```

Document each macro with comments explaining purpose and parameters.
  </action>
  <verify>Run `cd dbt && dbt compile --select tag:macro_test` (or just verify files exist with correct syntax)</verify>
  <done>Three macros exist with documented Epley formula, anomaly detection, and gym filtering logic</done>
</task>

</tasks>

<verification>
- [ ] All staging models compile with `dbt compile`
- [ ] Macros use correct DuckDB SQL syntax
- [ ] 1RM formula correctly implements Epley (weight * (1 + reps/30))
- [ ] Anomaly threshold defaults to 0.50 (50%)
- [ ] Gym filter handles both global and gym-specific cases
</verification>

<success_criteria>
- 3 staging models parse workout events from JSON payload
- 3 macros provide reusable analytics calculations
- All files follow existing dbt project conventions
- `dbt compile` succeeds for all new models
</success_criteria>

<output>
After completion, create `.planning/phases/03-history-analytics/03-01-SUMMARY.md`
</output>
